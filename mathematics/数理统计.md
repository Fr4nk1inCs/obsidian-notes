#数学 #概率论 #统计
# 统计三大分布
## 卡方分布 $\chi^2$
### 定义
设 $X_1, X_2, \cdots, X_n$ 是来自标准正态总体 $N(0,1)$ 的一个样本, 令 $\chi^2 = X_1^2 + X_2^2 + \cdots + X_n^2$ 则称 $\chi^2$ 服从自由度为 $n$ 的卡方分布, 记为 $\chi^2\sim\chi^2_n$.
1. $X_1, X_2, \cdots, X_n$ 独立同分布且 $X_j\sim N(0,1)$
2. 如果 $X_1, X_2, \cdots, X_n$ 独立同分布 $X_j\sim N(\mu, \sigma^2)$, 则 $\frac{X_j-\mu}{\sigma}\sim N(0,1)$, 此时 $\sum^n_{j = 1}\left(\frac{X_j-\mu}{\sigma}\right)^2\sim \chi^2_n$

### 概率密度及其图形
$\chi^2$ 分布的概率密度函数为
$$
f(x) = \begin{cases}
    \displaystyle\frac{1}{\displaystyle2^{\frac n2}\Gamma\left(\frac n2\right)}x^{\frac n2-1}\exp\left(-\frac{x}{2}\right) & \text{if } x > 0 \\
    0 & \text{if } x \le 0
\end{cases}
$$
分布图形如下

![卡方分布图形](chi2.svg)

### 性质
1. **可加性**: 如果 $X\sim\chi^2_{n_1}, Y\sim\chi^2_{n_2}$, 并且 $X, Y$ 相互独立, 则 $X+Y\sim\chi^2_{n_1+n_2}$
2. 若 $\chi^2\sim\chi^2_n$, 则 $E(\chi^2)=n, Var(\chi^2) = 2n$

### 上侧分位点
设 $X\sim\chi^2_n$, 对给定的正数 $\alpha\ (0<\alpha<1)$, 称满足条件 $P(X>\chi^2_n(\alpha))=\alpha$ 的点 $\chi^2_n(\alpha)$ 为 $\chi^2$ 分布的上 $\alpha$ 分位点.
1. 随机变量 $X$ 落在 $\chi^2_n(\alpha)$ 右侧的概率等于 $\alpha$
2. 上 $\alpha$ 分位点 $\chi^2_n(\alpha)$ 可查 $\chi^2_n$ 分布表求得.
3. 当 $n\ge45$ 时, 费歇证明: $\sqrt{2\chi^2_n}\to N(\sqrt{2n-1}, 1)$, 故 $\chi^2_n(\alpha)\approx\frac12(z(\alpha)+\sqrt{2n-1})$, 其中 $z(\alpha)$ 为 $N(0,1)$ 的上 $\alpha$ 分位点

## *t* 分布 (Student 分布)
### 定义
设随机变量 $X$ 和 $Y$ 相互独立, 且 $X\sim N(0,1), Y\sim\chi^2_n$, 则称统计量
$$
T = \frac{X}{\sqrt{Y/n}}
$$
服从自由度为 $n$ 的 $t$ 分布, 记作 $T\sim t_n$

$t$ 分布的概率密度函数为
$$
f(x)=\frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2}\right)}\left(1+\frac{x^2}{n}\right)^{-\frac{n+1}{2}}\qquad x\in\mathbb{R}
$$
可以证明 
$$
\lim_{n\to\infty}f(x) =\frac{1}{\sqrt{n\pi}}\exp\left(-\frac{x^2}{2}\right)\qquad \forall x\in\mathbb{R}
$$
即 $n$ 充分大时, $t$ 分布以标准正态分布为极限分布.
### 主要特征
![t分布图形](t.svg)
1. 图形特征:
    a. 概率密度函数 $f(x)$ 是偶函数, 关于纵轴对称
    b. 当 $n>45$ 时, $t_n\to N(0,1)$
2. 数字特征
若 $T\sim t_n$, 则 $E(T) = 0, Var(T)=\frac{n}{n-2}\ (n>2)$
### 上侧分位点
设 $T\sim t_n$, 对给定的正数 $\alpha\ (0<\alpha<1)$, 称满足条件 $P(T>t_n(\alpha))=\alpha$ 的点 $t_n(\alpha)$ 为 $t$ 分布的上 $\alpha$ 分位点.
1. 随机变量 $T$ 落在 $t_n(\alpha)$ 右侧的概率等于 $\alpha$
2. 上 $\alpha$ 分位点 $t_n(\alpha)$ 可查 $t_n$ 分布表求得. $n>45$ 时, $t_n(\alpha)\approx z(\alpha)$
3. 由 $t$ 分布图形的对称性: $t_{n}(1-\alpha)=-t_n(\alpha)$

## *F* 分布 (Fisher 分布)

### 定义
设 $U\sim \chi^2_{n_1}, V\sim\chi^2_{n_2}$, 且 $U$ 与 $V$ 相互独立, 则称统计量 $F = \frac{U/n_1}{V/n_2}$ 服从自由度为 $(n_1, n_2)$ 的 $F$ 分布, 记 $F\sim F_{n_1, n_2}$.

$F$ 分布的概率密度函数为
$$
f(x) = \begin{cases}
\displaystyle \frac{\Gamma\left(\frac{n_1+n_2}{2}\right)}{\Gamma\left(\frac{n_1}{2}\right)\Gamma\left(\frac{n_2}{2}\right)}\left(\frac{n_1}{n_2}\right)^{\frac{n_1}{2}}x^{\frac{n_1}{2}-1}\left(1+\frac{n_1}{n_2}x\right)^{-\frac{n_1+n_2}{2}} & \text{if } x\ge0 \\
0 & \text{if } x<0
\end{cases}
$$

![F分布图形](f.svg)

### 主要性质
1. 若 $F\sim F_{n_1,n_2}$, 则 $\frac{1}F\sim F_{n_2,n_1}$
2. 若 $T\sim t_n$, 则 $T^2\sim F_{1,n}$

### 上侧分位点
设 $F\sim F_{n_1,n_2}$, 对给定的正数 $\alpha\ (0<\alpha<1)$, 称满足条件 $P(F>F_{n_1,n_2}(\alpha))=\alpha$ 的点 $F_{n_1,n_2}(\alpha)$ 为 $F$ 分布的上 $\alpha$ 分位点.
1. 随机变量 $F$ 落在 $F_{n_1,n_2}(\alpha)$ 右侧的概率等于 $\alpha$
2. $\displaystyle F_{n_1,n_2}(1-\alpha) = \frac{1}{F_{n_1,n_2}(\alpha)}$

## 正态总体的样本均值样本方差的分布
### 一般总体的样本均值, 样本方差的性质
设总体 $X$ 分布未知, 但 $E(X) = \mu, Var(X)=\sigma^2$, $X_1,X_2,\cdots,X_n$ 是来自总体 $X$ 的一个样本, $\overline{X}, S^2$ 是样本均值和样本方差, 则
1. $E(\overline{X}) = \mu, Var(\overline{X}) = \frac{\sigma^2}{n}$
2. $E(S^2)=Var(X)=\sigma^2$

### 正态总体的样本均值, 方差的分布
<font color = green> 定理 1.</font> 设 $X_1, X_2, \cdots, X_n$ 是来自正态总体 $N(\mu,\sigma^2)$ 的样本, 则
1. 样本均值 $\displaystyle\overline X\sim N(\mu,\frac{\sigma^2}{n})\Leftrightarrow \frac{\overline{X}-\mu}{\sigma/\sqrt n}\sim N(0,1)$
2. $\displaystyle\frac{(n-1)S^2}{\sigma^2}=\sum^n_{j=1}(\frac{X_j-\overline{X}}{\sigma})^2\sim \chi^2_{n-1}$
3. $\overline X$ 与 $S^2$ 相互独立

<font color = green> 定理 2.</font> 设 $X_1, X_2, \cdots, X_n$ 是来自正态总体 $N(\mu,\sigma^2)$ 的样本, $\overline{X}, S$ 是样本均值和样本标准差, 则
$$
\frac{\overline X - \mu}{S - \sqrt{n}} = \frac{\frac{\overline X - \mu}{\sigma/\sqrt n}}{\sqrt{\frac{(n-1)S^2}{\sigma^2}/(n-1)}}\sim t_{n-1}
$$

<font color = green> 定理 3.</font> 设 $X_1, X_2, \cdots, X_n$ 是来自正态总体 $X\sim N(\mu,\sigma^2)$ 的样本, $Y_1, Y_2, \cdots, Y_n$ 是来自正态总体 $Y\sim N(\mu,\sigma^2)$ 的样本, 且 $X$ 与 $Y$ 相互独立. $\overline{X}, \overline{Y}$ 分别表示样本均值, $S_1^2, S_2^2$ 分别表示 $X$ 和 $Y$ 的样本方差, 则
1. $\displaystyle\frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}\sim F_{n_1-1, n_2-2}$
2. 当 $\sigma_1^2 = \sigma_2^2$ 时, $\displaystyle\frac{\overline{X}-\overline{Y} - (\mu_1-\mu_2)}{S_w\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim t_{n_1+n_2-2}$, 其中 $\displaystyle S_w=\sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}}$

---

# 参数估计
## 总体和参数
### 总体, 个体和总体均值
- 在统计学中, 我们把所要调查对象的全体叫做**总体**, 把总体中的每个成员叫做**个体**.
- **总体参数**是描述总体特征的指标, 简称为**参数**.
    - **总体平均**是总体的平均值, 也叫**总体均值**, 常用 $\mu$ 表示总体均值. 当总体含有 $N$ 个个体, 第 $i$ 个个体是 $y_i$ 时, 有
    $$
    \mu=\frac{1}N\sum_{i=1}^N y_i
    $$
    - 当 $y_1, y_2, \cdots, y_N$ 是总体的全部个体, $\mu$ 是总体均值时, 定义**总体方差**或**方差**如下
    $$
    \sigma^2 = \frac1N\sum_{i=1}^N (y_i-\mu)^2
    $$
    - **总体标准差**是总体方差的算术平方根 $\sigma=\sqrt{\sigma^2}$, 简称为标准差.

### 样本与估计
- 从总体中抽取一部分个体, 称这些个体为**样本**, 也叫观测数据. 称从总体抽取样本的工作为**抽样**.
    - 称构成样本的个体数目为**样本容量**, 简称为**样本量**, 用 $n$ 表示.
    - **样本均值**是样本的平均值, 用 $\overline x$ 表示, 有
		   $$
    \overline x=\frac{1}{n}\sum_{i=1}^n x_i
    $$
    - 给定 $n$ 个样本 $x_1, x_2, \cdots, x_n$, 定义**样本方差**如下
    $$
    s^2 = \frac{1}{n-1}\sum^n_{i=1}(x_i-\overline x)^2
    $$
    - **样本标准差**是样本方差的算术平方根 $s=\sqrt{s^2}$.
- **估计**是利用样本计算出的对参数的估计值, 能从观测数据直接计算出来.

## 抽样调查
### 抽样调查的必要性
- 在很多实际问题中, 采用抽样的方法来确定总体性质不仅是必要的, 也是必须的.
- 总体很大时, 抽样调查往往可以提高调查的质量. 有人认为抽样调查不如全面调查得到的结论准确, 这是不客观的. 看到抽样调查是用局部推断全体, 带有抽样的误差, 只是看到了问题的一个方面. 实际上调查数据的质量更重要, 总体很大时进行全面调查, 往往因为工作量过大、时间过长等而影响数据的质量. 一项经过科学设计并严格实施的抽样调查可能得到比全面调查更可靠的结果.

### 随机抽样

- 如果总体中的每个个体都有相同的机会被抽中, 就称这样的抽样方法为**随机抽样**方法.人们经常用“**任取**”, “**随机抽取**”或“**等可能抽取**”等来表示随机抽样.
- 从概率论的知识知道, 如果从总体中任选一个个体, 这个个体是随机变量, 这个随机变量的数学期望是总体均值, 方差是总体方差.
- 随机抽样又分为无放回的随机抽样和有放回的随机抽样.无放回的随机抽样指在总体中随机抽出一个个体后, 下次在余下的个体中再进行随机抽样.有放回的随机抽样指抽出一个个体, 记录下抽到的结果后放回, 摇匀后再进行下一次随机抽样.
  - 在相同的总体中和相同的样本量下, 无放回随机抽样得到的结果比有放回随机抽样得到的结果要好. 但是当总体的数量很大, 样本量相对总体的数量又很小时, 这两种抽样方法得到的结果是相近的.
- 试验和理论都证明:在随机抽样下, 样本均值 $\overline x$ 是总体均值 $\mu$ 很好的估计, 样本标准差 $S$ 是总体标准差 $\sigma$ 很好的估计.在样本量不大时, 增加样本量可以比较好地提高估计的精确度.

### 随机抽样的无偏性
样本均值是对总体均值的估计.在总体中任取一个个体 $X$, $X$ 是随机变量, 从数学期望的定义知道 $E(X)=\mu$ 是总体均值.这说明随机抽样是无偏的. 如果用 $X_1,X_2,\cdots,X_n$ 表示依次随机抽取的样本, 则样本均值
$$
\overline X=\frac{1}{n}\sum_{i=1}^n X_i
$$
是总体均值 $\mu$ 的估计, 因为有
$$
E(\overline X) = \frac{1}{n}\sum_{i=1}^n E(X_i) = \frac{1}{n}\sum_{i=1}^n \mu = \mu
$$

## 样本均值和样本方差

> 如果 $X$ 是从总体中随机抽样得到的个体, 则 $X$ 是随机变量, $X$ 的分布就是总体的分布. 如果对总体进行有放回的随机抽样, 则得到独立同分布且和 $X$ 同分布的随机变量 $X_1, X_2, \cdots, X_n$. 这时称 $X_1, X_2, \cdots, X_n$ 是总体 $X$ 的**简单随机样本**, 简称为总体 $X$ 的**样本**.

<font color = green>定义 1.3.1</font> 如果 $X_1, X_2, \cdots, X_n$ 独立同分布且和 $X$ 同分布, 则称 $X$ 是体, 称 $X_1, X_2, \cdots, X_n$ 是总体 $X$ 的样本, 称观测数据的个数 $n$ 为样本量.

> 实际问题得到的总是样本 $X_1, X_2, \cdots , X_n$ 的观测值 $x_1, x_2, \cdots, x_n$, 这时也称 $x_1, x_2, \cdots, x_n$ 是总体 $X$ 的样本. 通常不把两者作区分.

在统计问题中, 总体 $X$ 的分布往往是已知的. 例如重复测量一个物体的重量时, 认为总体 $X\sim N(\mu,\sigma^2)$, 未参数是 $\mu,\sigma^2$, 问题是根据总体 $X$ 的样本 $X_1, X_2, \cdots, X_n$ 估计未知参数 $\mu,\sigma^2$.

<font color = green>定义 1.3.2</font> 设 $X_1, X_2, \cdots, X_n$ 是总体 $X$ 的样本, $\theta$ 是总体 $X$ 的未知参数. 如果
$$
g_n(X_1, X_2, \cdots, X_n)
$$
是已知函数, 且我们用它来估计 $\theta$, 则称
$$
\hat \theta_n = g_n(X_1, X_2, \cdots, X_n) 
$$
是 $\theta$ 的**估计量**, 简称为估计. 换句话说, 估计或估计量是从观测数据 $x_1, x_2, \cdots, x_n$ 能够直接计算的量.计算后得到的值称为估计值.估计量也称为统计量.

为了符号的简便, 下面会把统计量 $\hat\theta_n$ 简写为 $\hat\theta$, 即 $$\hat\theta_n\equiv\hat\theta$$

<font color = green>定义 1.3.3</font> 设 $\hat\theta$ 是 $\theta$ 的估计.
1. 如果 $E(\hat\theta) = \theta$, 则称 $\hat\theta$ 是 $\theta$ 的**无偏估计**.
2. 如果当样本量 $n\to\infty$, $\hat\theta$ 依概率收敛到 $\theta$, 则称 $\hat\theta$ 是 $\theta$ 的**相合估计**.
3. 如果当样本量 $n\to\infty$, $\hat\theta$ 依概率 1 收敛到 $\theta$, 则称 $\hat\theta$ 是 $\theta$ 的**强相合估计**.

由于以概率 1 收敛可以推出依概率收敛, 所以强相合估计一定是相合估计.一个估计起码应当是相合的, 否则我们不知道这个估计有什么用, 也不知道它到底估计谁.

### 样本均值
设总体均值 $\mu = E(X)$ 存在, $X_1,X_2, \cdots, X_n$ 是总体 $X$ 的样本. 均值 $\mu$ 的估计定义为
$$
\overline{X_n} = \frac{1}{n}\sum_{i=1}^n X_i
$$
由于 $\overline{X_n}$ 是从样本计算出来的, 所以是样本均值, 有如下性质:
1. $\overline{X_n}$ 是 $\mu$ 的无偏估计, 因为 $E(\overline{X_n})=\mu$;
2. $\overline{X_n}$ 是 $\mu$ 的强相合估计,  因为从强大数律得到 
$$\lim_{n\to\infty}\overline{X_n}=\mu\ \mathrm{a.s.}$$

注意到
$$
Var(\overline{X_n})=Var\left(\frac1n\sum^n_{i=1}X_i\right)=\sum^n_{i=1}Var\left(\frac{X_i}{n}\right)=\sum^n_{i=1}\frac{\sigma^2}{n^2}=\frac{\sigma^2}{n}
$$
说明在均方误差的意义下, $n$ 越大精度越高, 称 $\overline{X_n}$ **更有效**.

### 样本方差
给定总体 $X$ 的样本 $X_1, X_2, \cdots, X_n$, 以下用 $\hat\mu$ 表示样本均值, 于是
$$
\hat\mu=\overline{X_n}
$$
总体方差 $\sigma^2=Var(X)$ 的估计定义为
$$
S^2=\frac{1}{n-1}\sum^n_{i=1}(X_i=\hat\mu)^2
$$
由于 $S^2$ 是从样本计算出来的, 所以称为**样本方差**, 有如下性质:
1. $S^2$ 是 $\sigma^2$ 的无偏估计.
取定 $i$, 因为 $E(X_i-\hat\mu)=\mu-\mu=0$, 所以从 $X_1,X_2,\cdots,X_n$ 的独立性得到
$$
\begin{aligned}
  E((X_i-\hat\mu)^2)&=Var(X_i-\hat\mu)
  =Var\left(X_i-\frac{1}{n}\sum^n_{j=1}X_j\right)
  =Var\left[\left(1-\frac1n\right)X_i-\frac{1}{n}\sum_{j\not=i}X_j\right]\\
  &=\left(\frac{n-1}{n}\right)^2\sigma^2+\frac1{n^2}\sum_{j\not=i}\sigma^2
  =\frac{n-1}{n}\sigma^2
\end{aligned}\\
\Rightarrow E(S^2)=\frac{1}{n-1}\sum_{i=1}^nE((X_i-\hat\mu)^2)=\sigma^2
$$
2. $S^2$ 是 $\sigma^2$ 的强相合估计.
利用强大数律
$$
\hat\mu\to\mu\ \mathrm{a.s.}\quad和\quad\frac{1}{n-1}\sum^n_{i=1}(X_i^2)\to E(X^2)\ \mathrm{a.s.}
$$
得到
$$
\begin{aligned}
  \frac{1}{n-1}\sum^n_{i=1}(X_i-\hat\mu)^2&=\frac{1}{n-1}\sum^n_{i=1}(X_i^2-2X_i\hat\mu+\hat\mu^2)=\frac{1}{n-1}\left(\sum^n_{i=1}X_i^2-2n\left(\frac{1}{n}\sum^n_{i=1}X_i\right)\hat\mu+n\hat\mu^2\right)\\
  &=\frac{1}{n-1}\sum^n_{i=1}X_i^2-\frac{n}{n-1}\hat\mu^2
\end{aligned}\\
\Rightarrow E(X^2)-\mu^2=\sigma^2\ \mathrm{a.s.}
$$

### 样本标准差
由于 $S^2$ 是 $\sigma^2$ 的估计, 所以定义标准差 $\sigma$ 的估计为
$$
S=\sqrt{S^2}=\sqrt{\frac{1}{n-1}\sum^n_{i=1}(X_i-\hat\mu)^2}
$$
称 $S$ 为样本标准差, 有如下性质:
1. 当 $\sigma>0$, $S$ 不是 $\sigma$ 的无偏估计, 即 $E(S)\not=\sigma$. 实际上, $S$ 低估了 $\sigma$, 即 $E(S)<\sigma$.
因为没有不全为零的常数 $a,b$, 使得 $P(aS+b=0)=1$, 所以由内积不等式得到
$$
E(S)=E(S\cdot1)<\sqrt{E(S^2)E(1^2)}=\sqrt{\sigma^2}=\sigma
$$
2. $S$ 是 $\sigma$ 的强相合估计, 因为 $S^2\to\sigma^2\ \mathrm{a.s.}\Rightarrow S\to\sigma\ \mathrm{a.s.}$.

将上面的结果总结如下

<font color = green>定理 1.3.1</font> 设 $X_1, X_2, \cdots, X_n$ 是总体 $X$ 的样本, $\mu=E(X)$, $\sigma^2=Var(X)>0$, 则
1. 样本均值 $\overline{X_n}$ 是总体均值 $\mu$ 的强相合无偏估计.
2. 样本方差 $S^2$ 是总体方差 $\sigma^2$ 的强相合无偏估计.
3. 样本标准差 $S$ 是总体标准差 $\sigma$ 的强相合估计, 但是 $E(S)<\sigma$.

> <font color = red>例 1.3.1</font> 设 $X_1, X_2, \cdots, X_n$ 是总体 $X$ 的样本, 当 $\mu_k=E(X^k)$ 存在时, 试给出 $\mu_k$ 的强相合无偏估计.
> 因为 $X_1^k, X_2^k, \cdots, X_n^k$ 独立同分布且和 $X^k$ 同分布, 所以是总体 $X^k$ 的样本, 且 $\hat\mu_k=\frac{1}{n}\sum^n_{i=1}X_i^k$ 是 $\mu_k$ 的估计, 由定理 1.3.1 可知 $\hat\mu_k$ 是 $\mu_k$ 的强相合无偏估计.
> 称 $\mu_k=E(X^k)$ 为 $X$ 的 $k$ 阶**原点矩**, 称 $\hat\mu_k$ 为 $k$ 阶**样本原点矩**.

## 矩估计

1. 如果总体 $X$ 的分布函数 $F(x;\theta)$ 只有一个未知参数 $\theta$, 则 $\mu_1=E(X)$ 常和 $\theta$ 有关, 如果 $g(s)$ 是已知函数, 并且能
$$
\mu_1=E(X)\Rightarrow\theta=g(\mu_1)
$$
则 $\hat\theta=g(\hat\mu_1)$ 是 $\theta$ 的矩估计, 其中 $\hat\mu_1$ 是样本均值 (1 阶样本原点矩).
2. 如果总体 $X$ 的分布函数 $F(x;\theta_1,\theta_2)$ 有两个未知参数 $\theta_1,\theta_2$, 则 $\mu_1=E(X),\mu_2=E(X^2)$ 常和 $\theta_1,\theta_2$ 有关, 如果 $g_1(s,t), g_2(s,t)$ 是已知函数, 并且能
$$
\begin{cases}
  \mu_1=E(X)\\
  \mu_2=E(X^2)
\end{cases}\Rightarrow
\begin{cases}
  \theta_1 = g_1(\mu_1,\mu_2)\\
  \theta_2 = g_2(\mu_1,\mu_2)
\end{cases}
$$
则 $\hat\theta_1 = g_1(\hat\mu_1, \hat\mu_2), \hat\theta_2=g_2(\hat\mu_1,\hat\mu_2)$ 分别是 $\theta_1$ 和 $\theta_2$ 的矩估计.

> <font color = red>例 1.4.1</font> 设 $X$ 服从泊松分布 $P(\lambda)$, $x_1, x_2, \cdots, x_n$ 是 $n$ 个样本的观测值, 试估计参数 $\lambda$.
> 因为 $X\sim P(\lambda)$, 所以 $\lambda = E(X) = \mu_1$, 而 $\mu_1$ 的矩估计是 $\hat\mu_1$, 则 $\lambda$ 的矩估计也是 $\hat\mu_1$, 得到 $\lambda$ 的估计
> $$
> \hat\lambda = \hat\mu_1 = \frac{1}{n}\sum^n_{i=1}x_i
> $$ 
> $\hat\lambda$ 即为矩估计.

## 最大似然估计
### 离散分布的情况
<font color = green>定义 1.5.1</font> 设离散随机变量 $X_1, X_2, \cdots, X_n$ 有联合分布
$$
p(x_1, x_2, \cdots, x_n;\theta) = P(X_1 = x_1, X_2 = x_2, \cdots, X_n = x_n)
$$
其中 $\theta$ 是未知参数, 给定观测数据 $x_1, x_2, \cdots, x_n$ 后, 称 $\theta$ 的函数
$$
L(\theta) = p(x_1, x_2, \cdots, x_n;\theta)
$$
为似然函数, 称 $L(\theta)$ 的最大值点 $\hat\theta$ 为 $\theta$ 的**最大似然估计**. 
- 其中的 $\theta$ 也可以是向量 $\boldsymbol\theta=(\theta_1, \theta_2, \cdots, \theta_m)$.
- 最大似然估计通常被缩写成 **MLE** (Maximum Likelihood Estimator).
- 因为 $\ln x$ 是严格单调的增函数, 所以 $I(\theta)=\ln L(\theta)$ 和 $L(\theta)$ 有相同的最大值点, 通常称 $I(\theta)$ 为**对数似然函数**. 在许多情况下, 最大似然估计可由**似然方程**
$$
L'(\theta) = 0\quad 或\quad I'(\theta) = 0
$$
解出.
### 连续分布的情况
<font color = green>定义 1.5.2</font> 设随机向量 $\boldsymbol X = (X_1, X_2, \cdots, X_n)$ 有联合密度 $f(\boldsymbol x; \boldsymbol\theta)$, 其中 $\boldsymbol\theta=(\theta_1, \theta_2, \cdots, \theta_m)$ 是未知参数. 在得到 $\boldsymbol X$ 的观测值 $\boldsymbol x = (x_1, x_2, \cdots, x_n)$ 后, 称
$$
L(\boldsymbol \theta) = f(\boldsymbol x; \boldsymbol\theta)
$$ 
为 $\boldsymbol\theta$ 的似然函数, 称 $L(\boldsymbol \theta)$ 的最大值点 $\hat{\boldsymbol\theta}$ 为 $\boldsymbol\theta$ 的**最大似然估计**.

设总体 $X$ 有概率密度 $f(x;\boldsymbol\theta)$, 则 $X$ 的样本 $X_1, X_2, \cdots, X_n$ (独立同分布) 有联合密度
$$
f(x_1, x_2, \cdots, x_n;\boldsymbol\theta) = \prod_{j=1}^n f(x_j;\boldsymbol\theta)
$$
基于观测值 $x_1, x_2, \cdots, x_n$ 的似然函数和对数似然函数是
$$
L(\boldsymbol\theta) = \prod_{j=1}^n f(x_j;\boldsymbol\theta) \qquad I(\boldsymbol\theta) = \sum_{j=1}^n \ln f(x_j;\boldsymbol\theta)
$$
那么求 $L(\boldsymbol\theta)$ 的最大值点可以通过解**似然方程组**
$$
\frac{\partial I(\boldsymbol\theta)}{\partial \theta_j} = 0\quad j = 1, 2, \cdots, m
$$
得到.

## 参数的区间估计

> 在独立同分布场合, 样本均值 $\overline{X_n}$ 和样本方差 $S^2$ 分别是总体均值 $\mu$ 和总体方差 $\sigma^2$ 的无偏估计和强相合估计, 说明样本均值和样本方差都是不错的估计量.它告诉我们, 在 $n$ 比较大的时候, 真值 $\mu$ 就在 $\overline{X_n}$ 附近, 真值 $\sigma^2$ 就在 $S^2$ 附近.但是到底离真值有多近呢? $n$ 多大就够了呢? 区间估计可以回答这一问题.

一般方法:
1. 找参数 $\theta$ 的点估计 $\hat\theta(X_1, X_2, \cdots, X_n)$
2. 找枢轴变量 $S(\theta, \hat\theta)$, 其分布与 $\theta, \hat\theta$ 无关.
3. 求 $S$ 的分位点, 反解出区间.

### 一个正态总体的区间估计

|情况|枢轴变量|分布|置信水平为 $1-\alpha$ 的双侧置信区间|单侧置信上下限|
|-|-|-|-|-|
|已知 $\sigma$, 找 $\mu$ 的置信区间|$\displaystyle Z=\frac{\sqrt{n}(\overline{X_n}-\mu)}{\sigma}$|$N(0,1)$|$\displaystyle\left[\overline{X_n}-\frac{z(\alpha/2)\sigma}{\sqrt{n}},\overline{X_n}+\frac{z(\alpha/2)\sigma}{\sqrt{n}}\right]$|$\displaystyle\overline{X_n}-\frac{z(\alpha)\sigma}{\sqrt{n}},\overline{X_n}+\frac{z(\alpha)\sigma}{\sqrt{n}}$|
|未知 $\sigma$, 找 $\mu$ 的置信区间|$\displaystyle T=\frac{\sqrt{n}(\overline{X_n}-\mu)}{S}$|$t_{n-1}$|$\displaystyle\left[\overline{X_n}-\frac{t_n(\alpha/2)S}{\sqrt{n}},\overline{X_n}+\frac{t_n(\alpha)S}{\sqrt{n}}\right]$|$\displaystyle\overline{X_n}-\frac{t_n(\alpha/2)S}{\sqrt{n}},\overline{X_n}+\frac{t_n(\alpha)S}{\sqrt{n}}$|
|已知 $\mu$, 找 $\sigma^2$ 的置信区间|$\displaystyle\chi^2=\frac{1}{\sigma^2}\sum^n_{j=1}(X_j-\mu)^2$|$\chi^2_n$|$\displaystyle\left[\frac{\sum^n_{j=1}(X_j-\mu)^2}{\chi_{n}^2(\alpha/2)},\frac{\sum^n_{j=1}(X_j-\mu)^2}{\chi_{n}^2(1-\alpha/2)}\right]$|$\displaystyle\frac{\sum^n_{j=1}(X_j-\mu)^2}{\chi_{n}^2(\alpha)},\frac{\sum^n_{j=1}(X_j-\mu)^2}{\chi_{n}^2(1-\alpha)}$|
|未知 $\mu$, 找 $\sigma^2$ 的置信区间|$\displaystyle\chi^2=\frac{1}{\sigma^2}\sum^n_{j=1}(X_j-\overline{X_n})^2$|$\chi^2_{n-1}$|$\displaystyle\left[\frac{(n-1)S^2}{\chi_{n-1}^2(\alpha/2)},\frac{(n-1)S^2}{\chi_{n-1}^2(1-\alpha/2)}\right]$|$\displaystyle\frac{(n-1)S^2}{\chi_{n-1}^2(\alpha)},\frac{(n-1)S^2}{\chi_{n-1}^2(1-\alpha)}$|

## 两个正态总体的区间估计
已知 $X, Y$ 相互独立.
### 均值差 $\mu_X-\mu_Y$ 的置信区间

|情况|枢轴变量|分布|
|-|-|-|
|$\sigma$ 已知|$\displaystyle Z=\frac{(\overline{X_n}-\overline{Y_m})-(\mu_X-\mu_Y)}{\displaystyle\sqrt{\frac{\sigma_X^2}{n}+\frac{\sigma_Y^2}{m}}}$|$N(0,1)$|
|$\sigma$ 未知但 $\sigma_X=\sigma_Y=\sigma$|$\displaystyle S_w^2=\frac{(n-1)S_X^2+(m-1)S_Y^2}{n+m-2}, T=\frac{(\overline{X_n}-\overline{Y_m})-(\mu_X-\mu_Y)}{\displaystyle S_w\sqrt{\frac{1}{n}+\frac{1}{m}}}$|$t_{n+m-2}$| 
|$\sigma$ 未知但 $\sigma_X^2/\sigma_Y^2=b^2$|$\displaystyle S_b^2=\frac{(n-1)S_X^2/b^2+(m-1)S_Y^2}{n+m-2}, T=\frac{(\overline{X_n}-\overline{Y_m})-(\mu_X-\mu_Y)}{\displaystyle S_b\sqrt{\frac{b^2}{n}+\frac{1}{m}}}$|$t_{n+m-2}$|

### 方差比 $\sigma_X^2/\sigma_Y^2$ 的置信区间
$\mu$ 未知时, 取枢轴变量为

$$
F=\frac{S_X^2/\sigma_X^2}{S_Y^2/\sigma_Y^2}\sim F_{n-1,m-1}
$$

对于 $F$ 分布, 有

$$
F_{n-1,m-1}(\alpha) = F_{m-1,n-1}(1-\alpha)
$$

---

# 假设检验
> 假设检验是统计推断的主要内容之一在科学研究, 日常工作甚至日常生活中有时会对某一件事情提出疑问. 澄清疑问的过程往往是先作一个和疑问相关的假设, 然后在这个假设下去寻找相关的证据. 如果得到的证据和假设相矛盾, 就拒绝这个假设.

## 假设检验的概念
> <font color = red>例 2.1</font> 一条新建的南北交通干线全长 10 km. 公路穿过一个隧道 (长度忽略不计), 隧道南面公路长 3.5 km, 北面公路长 6.5 km.在 刚刚通车的一个月中, 隧道南发生了 3 起互不相关的交通事故, 而隧道北没有发生交通事故, 能否认为隧道南的路面更容易发生交通事故?
> 
> 用 $p$ 表示一起交通事故发生在隧道南面的概率, 如果没起交通事故在这 10 km 的路面上等可能地发生, 则 $p = 0.35$, 于是 $p>0.35$ 表示隧道南的路面更容易发生交通事故. 
> 为了判断 $p>0.35$ 是否成立, 先作假设 $H_0:\ p=0.35$, 再作假设 $H_1:\ p>0.35$. 
> 在本问题中, 如果拒绝 $H_0$, 就应当接受 $H_1$. 注意这里 $H_1$ 和已有的事实相符, $H_0$ 和已有事实相悖.
> 用 $W$ 表示三起交通事故都发生在隧道南, 如果 $H_0$ 为真, 因为三起交通事故是相互独立的, 所以 $P(W)=0.35^3\approx4.29\%$, 这是一个小概率事件, 一般认为不会发生. 因为小概率事件 $W$ 发生了, 所以要拒绝 $H_0$, 因此认为隧道南的路面更容易发生交通事故.

在上面的例子中, 称 $H_0$ 为**原假设**或**零假设**, 而 $H_0$ 的对立面 $H_1$ 称为**备择假设**. 做出 "隧道南的路面更容易发生交通事故" 的结论也有可能犯错误, 而犯错误的概率正是 $P(W|H_0)\approx4.29\%$. 拒绝原假设 $H_0$ 时犯的这类错误被称为**第一类错误**. 而犯第一类错误的原因正是小概率事件 $W$ 的发生, 因此称 $W$ 为**拒绝域**或**否定域**.

下面给出假设检验的概念:
1. 进行假设检验时, 根据问题的背景, 先做出**原假设** $H_0$ 及其**备择假设** $H_1$.
2. 在 $H_0$ 下, 计算观测数据出现的概率 $P$
	1. 如果 $P$ 很小, 就拒绝 $H_0$, 接受 $H_1$.
	2. 如果 $P$ 不是很小, 也不必急于接受 $H_0$.

为了简便, 我们把以上的原假设与备择假设记作

$$
H_0:\ p=0.35\quad v.s. \quad H_1:\ p>0.35
$$

在解决假设检验的问题时, 无论作出拒绝还是接受原假设 $H_0$ 的决定, 都有可能犯错误. 在统计学中, 称拒绝 $H_0$ 时犯的错误为第一类错误, 称接受 $H_0$ 时犯的错误为第二类错误. 具体如下:
1. $H_0$ 为真, 统计推断的结果拒绝 $H_0$, 犯第一类错误;
2. $H_0$ 为假, 统计推断的结果接受 $H_0$, 犯第二类错误;
3. $H_0$ 为真, 统计推断的结果没有拒绝 $H_0$, 不犯错误;
4. $H_0$ 为假, 统计推断的结果拒绝 $H_0$, 不犯错误.

## 显著性检验
在应用问题中, 常用到**显著性检验**. 显著性检验的任务是依据观测数据或试验数据判断原假设 $H_0$ 是否成立: 如果观测数据或试验数据和原假设有显著的差异, 就拒绝 $H_0$, 并称**检验显著**. 否则不能拒绝 $H_0$, 并称**检验不显著**. 注意, **不能拒绝 $H_0$ 并不表示一定要接受 $H_0$**. 
- 显著性检验的原则是控制犯第一类错误的概率不超过某个定值 $\alpha$, 比如说 $\alpha=0.05$, 而对犯第二类错误的概率没有限制. 因此, 显著性检验对原假设 $H_0$ 有利.
- 我们通常把想证明的结论放在备择假设 $H_1$, 然后去拒绝原假设 $H_0$. 
### 一样本正态总体的显著性检验
- 双边检验: $H_0: m = m_0\quad v.s.\quad H_1: m\neq m_0$ ($m$ 为统计量)
- 单边检验: $H_0: m > m_0 (m < m_0)\quad v.s. \quad H_1: m < m_0 (m > m_0)$

下表展示了一样本正态总体的假设检验方案, 其中拒绝域自上到下分别为假设检验

$$
\begin{cases}
H_0: m = m_0\quad v.s.\quad H_1: m\neq m_0\\
H_0: m < m_0\quad v.s.\quad H_1: m > m_0\\
H_0: m > m_0\quad v.s.\quad H_1: m < m_0
\end{cases}
$$

的拒绝域.

|检验对象|检验统计量|分布|拒绝域|
|-|-|-|-|
|$\mu$ ($\sigma^2$ 已知)|$\displaystyle Z=\frac{\sqrt{n}(\overline{X}-\mu_0)}{\sigma}$|$N(0,1)$|$\begin{cases}\lvert Z\rvert > z_{\alpha/2}\\ Z>z_\alpha\\ Z<-z_\alpha\end{cases}$|
|$\mu$ ($\sigma^2$ 未知)|$\displaystyle T=\frac{\sqrt{n}(\overline{X}-\mu_0)}{S}$|$t_{n-1}$|$\begin{cases}\lvert T\rvert > t_{n-1}(\alpha/2)\\ T>t_{n-1}(\alpha)\\ T<-t_{n-1}(\alpha)\end{cases}$|
|$\sigma^2$ ($\mu$ 已知)|$\displaystyle\chi^2=\frac{1}{\sigma^2}\sum^n_{j=1}(X_j-\mu)^2$|$\chi^2_n$|$\begin{cases}\chi^2 > \chi^2_n(\alpha/2)\text{或者}\chi^2 < \chi^2_n(1-\alpha/2)\\\chi^2 > \chi^2_n(\alpha)\\\chi^2 < \chi^2_n(1-\alpha)\end{cases}$|
|$\sigma^2$ ($\mu$ 未知)|$\displaystyle\chi^2=\frac{1}{\sigma^2}\sum^n_{j=1}(X_j-\overline{X})^2$|$\chi^2_{n-1}$|$\begin{cases}\chi^2 > \chi^2_{n-1}(\alpha/2)\text{或者}\chi^2 < \chi^2_{n-1}(1-\alpha/2)\\\chi^2 > \chi^2_{n-1}(\alpha)\\\chi^2 < \chi^2_{n-1}(1-\alpha)\end{cases}$|
### 两样本均值比较的显著性检验
#### 已知 $\sigma_1^2,\sigma_2^2$ 时, $\mu_1,\mu_2$ 的检验
取检验统计量为

$$
Z=\frac{\overline{X}-\overline{Y}}{\displaystyle\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\sim N(0,1)
$$

则假设检验

$$
\begin{cases}
H_0: \mu_1 = \mu_2\quad v.s.\quad H_1: \mu_1\neq \mu_2\\
H_0: \mu_1 < \mu_2\quad v.s.\quad H_1: \mu_1 > \mu_2\\
H_0: \mu_1 > \mu_2\quad v.s.\quad H_1: \mu_1 < \mu_2
\end{cases}
$$

的拒绝域为

$$
\begin{cases}
\lvert Z\rvert > z_{\alpha/2}\\
Z>z_\alpha\\
Z<-z_\alpha
\end{cases}
$$

#### 已知 $\sigma_1 = \sigma_2$ 时, $\mu_1,\mu_2$ 的检验
取检验统计量为

$$
T=\frac{\overline{X}-\overline{Y}}{\displaystyle S_w\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim t(n_1+n_2-2),\ S_w^2=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}
$$

则假设检验

$$
\begin{cases}
H_0: \mu_1 = \mu_2\quad v.s.\quad H_1: \mu_1\neq \mu_2\\
H_0: \mu_1 < \mu_2\quad v.s.\quad H_1: \mu_1 > \mu_2\\
H_0: \mu_1 > \mu_2\quad v.s.\quad H_1: \mu_1 < \mu_2
\end{cases}
$$

的拒绝域为

$$
\begin{cases}
\lvert T\rvert > t_{n_1+n_2-2}(\alpha/2)\\
T>t_{n_1+n_2-2}(\alpha)\\
T<-t_{n_1+n_2-2}(\alpha)
\end{cases}
$$

#### 成对数据的假设检验
记两对数据为 $\left(\begin{matrix}X\\Y\end{matrix} \right)$, 作差 $Z=X-Y$, 则 $Z\sim N(\mu=\mu_1-\mu_2, \sigma^2)$, 问题转化为一个总体的 $t$ 检验问题. 

#### ^*^ 未知 $\sigma_1^2, \sigma_2^2$ 时, $\mu_1,\mu_2$ 的大样本检验
当 $n_1, n_2$ 较大时, $S_1^2\approx\sigma_1^2, S_2^2\approx\sigma_2^2$, 因此用 $S_1^2, S_2^2$ 代替已知 $\sigma^2$ 时的 $\sigma_1^2, \sigma_2^2$, 得到检验统计量

$$
Z=\frac{\overline{X}-\overline{Y}}{\displaystyle\sqrt{\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2}}}\sim N(0,1)
$$

近似成立, 当 $\mu_1 = \mu_2$ 时, 就用 $Z\sim N(0,1)$ 作为假设检验的依据.
### 两样本方差的显著性检验

下表展示了两样本正态总体方差的假设检验方案, 其中拒绝域自上到下分别为假设检验

$$
\begin{cases}
H_0: \sigma_1^2 = \sigma_2^2\quad v.s.\quad H_1: \sigma_1^2\neq \sigma_2^2\\
H_0: \sigma_1^2 < \sigma_2^2\quad v.s.\quad H_1: \sigma_1^2 > \sigma_2^2\\
H_0: \sigma_1^2 > \sigma_2^2\quad v.s.\quad H_1: \sigma_1^2 < \sigma_2^2
\end{cases}
$$

的拒绝域.

|情况|检验统计量|分布|拒绝域|
|-|-|-|-|
|均值已知|$\displaystyle F=\frac{\frac{1}{n_1}\sum^{n_1}_{i=1}(\overline{X}-\mu_1)^2}{\frac{1}{n_2}\sum^{n_2}_{i=1}(\overline{Y}-\mu_2)^2}$|$F_{n_1, n_2}$|$\begin{cases}F>F_{n_1,n_2}(\alpha/2)\text{或者}F<F_{n_1, n_2}(1-\alpha/2)\\F>F_{n_1, n_2}(\alpha)\\F<F_{n_1, n_2}(1-\alpha)\end{cases}$|
|均值未知|$\displaystyle F=\frac{S_1^2}{S_2^2}$|$F_{n_1-1, n_2-1}$|$\begin{cases}F>F_{n_1-1,n_2-1}(\alpha/2)\text{或者}F<F_{n_1-1, n_2-1}(1-\alpha/2)\\F>F_{n_1-1, n_2-1}(\alpha)\\F<F_{n_1-1, n_2-1}(1-\alpha)\end{cases}$|

## 拟合优度检验
拟合优度检验考虑的是观测样本样本及其总体分布是否能够拟合, 以及拟合好坏的标准. 设 $X_1, X_2, \cdots , X_n$ 是总体 $X$ 的样本, 对于已知的概率分布函数 $F(x)$, 考虑假设
$$
H_0:X\sim F(x)\quad v.s. \quad H_1:F(x) 不是 X 的分布函数
$$
的检验问题.

给定总体 $X$ 的观测值 $x_1, x_2, \cdots, x_n$, 取
$$
t_0 < \min\{x_1, x_2,\cdots,x_n\}\qquad t_m>\max\{x_1, x_2, \cdots, x_n\}
$$ 
类似于制作频率直方图的方法, 取 $t_0<t_1<t_2<\cdots<t_m$, 将区间 $(t_0,t_m]$ 划分为如下不相交的区间 $I_j$
$$
I_j = (t_{j-1}, t_j], j=1,2,\cdots,m
$$
用观测样本落入区间 $I_j$ 的概率
$$
\hat p_j=\frac{^\#\{k|x_k\in I_j\}}{n} = \frac{1}{n}\sum^n_{k=1}\mathrm{I}[x_k\in I_j]
$$
作为概率 $p_j=P(X\in I_j)=F(t_j)-F(t_{j-1})$ 的估计.
用
$$
Z=\sum^m_{j=1}\frac{n}{p_j}(p_j-\hat p_j)^2=\sum^m_{j=1}\frac{(np_j-n\hat p_j)^2}{np_j}=\sum\frac{(理论值-经验值)^2}{理论值}
$$
皮尔逊在1900年证明了: 如果原假设 $H_0$ 成立, 在样本大小 $n\to\infty$ 时, $Z$ 的分布趋向于 $\chi^2_{m-1}$ 分布. 
于是 $H_0$ 的显著水平 (近似) 为 $\alpha$ 的拒绝域为
$$
W=\{Z>\chi^2_{m-1}(\alpha)\}
$$
如果总体分布 $F(x;\theta_1, \theta_2,\cdots,\theta_r)$ 有 $r$ 个未知参数, 则需要用观测数据先计算出这 $r$ 个参数的最大似然估计, 用最大似然估计代替真实参数后才能计算出 $p_j$, 可以证明: 如果原假设 $H_0$ 成立, 在样本大小 $n\to\infty$ 时, $Z$ 的分布趋向于 $\chi^2_{m-r-1}$ 分布.于是 $H_0$ 的显著水平 (近似) 为 $\alpha$ 的拒绝域为
$$
W=\{Z>\chi^2_{m-r-1}(\alpha)\}
$$
将拒绝 $H_0$ 犯错误的概率 $P=P(\chi^2_{m-1/m-r-1}\ge Z)$ 称为**拟合优度**, 它反映了数据与分布 $F(x)$ 的拟合情况, 拟合优度越大, 拟合程度越好.

实际应用中还需要要求样本量和区间划分满足下面的条件
$$
np_j\ge5,j=1,2,\cdots,m
$$

## 列联表的独立性检验
### 2×2 列联表
设随机向量 $(X,Y)$ 有概率分布和边缘分布如下

|  $X,Y$   |  $Y=1$   |  $Y=2$   | $P(X=i)$ |
|:--------:|:--------:|:--------:|:--------:|
|  $X=1$   | $p_{11}$ | $p_{12}$ |  $p_1$   |
|  $X=2$   | $p_{21}$ | $p_{22}$ |  $p_2$   |
| $P(Y=j)$ |  $q_1$   |  $q_2$   |          |

这时需要解决的问题是检验以下假设

$$
H_0:X,Y独立\quad v.s. \quad H_1:X,Y不独立
$$

记观测值如下

| $X,Y$ |           $Y=1$            |           $Y=2$            | 合计                            |
|:-----:|:--------------------------:|:--------------------------:| ------------------------------- |
| $X=1$ |          $n_{11}$          |          $n_{12}$          | $n_{1\cdot}=n_{11}+n_{12}$      |
| $X=2$ |          $n_{21}$          |          $n_{22}$          | $n_{2\cdot}=n_{21}+n_{22}$      |
| 合计  | $n_{\cdot1}=n_{11}+n_{21}$ | $n_{\cdot2}=n_{12}+n_{22}$ | $n=n_{11}+n_{12}+n_{21}+n_{22}$ |

得到 $(X,Y)$ 的频率分布为

|   $X,Y$    |                    $Y=1$                     |                    $Y=2$                     |                   $\hat p_i$                   |
|:----------:|:--------------------------------------------:|:--------------------------------------------:|:----------------------------------------------:|
|   $X=1$    | $\displaystyle\hat p_{11}=\frac{n_{11}}{n}$  | $\displaystyle\hat p_{12}=\frac{n_{12}}{n}$  | $\displaystyle\hat p_1 = \frac{n_{1\cdot}}{n}$ |
|   $X=2$    | $\displaystyle\hat p_{21}=\frac{n_{21}}{n}$  | $\displaystyle\hat p_{22}=\frac{n_{22}}{n}$  | $\displaystyle\hat p_2 = \frac{n_{2\cdot}}{n}$ |
| $\hat q_j$ | $\displaystyle\hat q_1=\frac{n_{\cdot1}}{n}$ | $\displaystyle\hat q_2=\frac{n_{\cdot2}}{n}$ |                                                |

在 $H_0$ 下, $X, Y$ 独立, 故有 $p_{ij} = p_iq_j$. 频率作为概率的强相合估计, 这时也应该有

$$
\hat p_{ij}=p_iq_j
$$

于是当 

$$
V_n=\sum_{i=1}^{2}\sum_{j=1}^2\frac{n(p_{ij}-\hat p_i\hat q_j)^2}{\hat p_i\hat q_j}
$$

取值较大时应当拒绝 $H_0$, 即 $X,Y$ 不独立.

在数学上可以证明, 如果 $H_0$ 成立, 则 $V_n$ 将依分布收敛到 $\chi^2_1$, 因此 $H_0$ 显著水平 (近似) 为 $\alpha$ 的拒绝域为

$$
W_\alpha = \{V_n\ge \chi^2_1(\alpha)\}
$$

实际计算时, 有

$$
V_n=\frac{n(n_{11}n_{22}-n_{12}n_{21})^2}{n_{1\cdot}n_{2\cdot}n_{\cdot1}n_{\cdot2}}
$$

### *k*×*l* 列联表

同样, 记 

$$
  n_{i\cdot} = \sum_{j=1}^l n_{ij}\qquad n_{\cdot j}=\sum_{i=1}^k n_{ij}\\
  n = \sum_{i=1}^k n_{i\cdot} = \sum_{j=1}^l n_{\cdot j} = \sum_{i = 1}^k \sum_{j = 1}^l n_{ij}
$$

那么频率

$$
\hat p_{ij} = \frac{n_{ij}}{n}\quad \hat p_i = \frac{n_{i\cdot}}{n}\quad \hat q_j = \frac{n_{\cdot j}}{n}
$$

检验以下假设

$$
H_0:X,Y独立\quad v.s. \quad H_1:X,Y不独立
$$

同 2×2 列联表, 记

$$
V_n = \sum_{i=1}^{k}\sum_{j=1}^{l}\frac{n(p_{ij}-\hat p_i\hat q_j)^2}{\hat p_i\hat q_j}
$$

取值较大时应当拒绝 $H_0$, 即 $X,Y$ 不独立.

在数学上可以证明, 如果 $H_0$ 成立, 则 $V_n$ 将依分布收敛到 $\chi^2_{(k-1)(l-1)}$, 因此 $H_0$ 显著水平 (近似) 为 $\alpha$ 的拒绝域为

$$
W_\alpha = \{V_n\ge \chi^2_{(k-1)(l-1)}(\alpha)\}
$$

实际计算时, 有

$$
V_n=n\left(\sum^k_{i=1}\sum^l_{j=1}\frac{n_{ij}^2}{n_{i\cdot}n_{\cdot j}}-1\right)
$$